{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269e50b-edd4-47c7-b0bf-2d50957cddd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35359b11-e771-4468-af80-c42229439e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\USER\\\\Documents\\\\PapersforSelfDevelopment\\\\PaperSD1\\\\DataSet.csv\")\n",
    "#df=pd.read_csv(\"C:\\\\Users\\\\USER\\\\Documents\\\\CleanedDatasetUsed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304e564-c068-4c82-97a2-400d8c8a8909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a468a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()\n",
    "correlations = df.corr()\n",
    "correlations = pd.DataFrame(correlations)\n",
    "correlations.to_csv('correlationsoftheCSTSFSdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd145f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat= df.describe()\n",
    "stat.to_csv('DescriptionStat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa048b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4485e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e57332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('TS', axis=1)\n",
    "y = df['TS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50baa5-3b87-4d80-8b92-47cad2219567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check the shape of the data at this point\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0902c-529b-491e-994d-88c9d380092e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#information on the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5f0ab-6613-4215-831e-8934cb9d5ef6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#statistics about the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9eaf86-6e72-4b88-9364-0e0b55a8f7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4f041-57ae-4c6d-8a9f-4aab633ee7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#EDA commences here\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(13,4))\n",
    "    sns.boxplot(x=column,data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddbc55-e495-4f31-8ab7-a24eaa13b5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#produce a heatmap of the above result\n",
    "plt.figure(figsize=(11,9))\n",
    "sns.heatmap(data=df.corr(),annot=True)\n",
    "plt.savefig('correlation coefficient.tif', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702da4a3-312a-4615-a39e-4f9a4cc3087b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#conduct a univariate analysis\n",
    "\n",
    "#make a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram plots\n",
    "axes = df.hist(figsize=(11, 9))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Distribution.tif', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to make data normally distributed, split data first in train-val-test\n",
    "\n",
    "X=df.drop(['CS'],axis=1) #X is \"remove the variable in '' \"\n",
    "y=df['CS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a901e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_model_train, X_test, y_model_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be15449",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_train.to_csv(\"TrainingY_70-30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_train.to_csv('yfortrain30percent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537706c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18996de1-bae0-4bbc-9eb9-9fe0f2377f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainingset=pd.read_csv(\"C:\\\\Users\\\\USER\\\\Documents\\\\PapersforSelfDevelopment\\\\PaperSD1\\\\CSandFSandTS\\\\TrainingSet_70-30-CS.csv\")\n",
    "Testset=pd.read_csv(\"C:\\\\Users\\\\USER\\\\Documents\\\\PapersforSelfDevelopment\\\\PaperSD1\\\\CSandFSandTS\\\\TestingSet_70-30-CS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01945f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingsetCB=pd.read_csv(\"C:\\\\Users\\\\USER\\\\Documents\\\\PapersforSelfDevelopment\\\\PaperSD1\\\\CSandFSandTS\\\\TrainingSetfor_70-30.csv\")\n",
    "TestsetCB=pd.read_csv(\"C:\\\\Users\\\\USER\\\\Documents\\\\PapersforSelfDevelopment\\\\PaperSD1\\\\CSandFSandTS\\\\TestingSetfor_70-30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conduct a univariate analysis\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 12})\n",
    "#make a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram plots\n",
    "axes = TrainingsetCB.hist(figsize=(11, 9))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('TrainingsetCB.tif', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conduct a univariate analysis\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 12})\n",
    "#make a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram plots\n",
    "axes = TestsetCB.hist(figsize=(11, 9))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('TestsetCB.tif', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainingset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "train_data = Trainingset\n",
    "test_data = Testset\n",
    "\n",
    "predictor = TabularPredictor(label='CS').fit(train_data=train_data)\n",
    "predictions = predictor.predict(test_data.drop(columns='CS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(train_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4837b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d463ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "FSResults=predictor.leaderboard(train_data, extra_metrics=['r2', 'mean_squared_error', 'mse', 'root_mean_squared_error', 'mean_absolute_error', 'median_absolute_error', \n",
    "                                                'mean_absolute_percentage_error', 'symmetric_mean_absolute_percentage_error', 'spearmanr', 'pearsonr'])\n",
    "FSResults.to_csv('FSResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "FSTestResults=predictor.leaderboard(test_data, extra_metrics=['r2', 'mean_squared_error', 'mse', 'root_mean_squared_error', 'mean_absolute_error', 'median_absolute_error', \n",
    "                                                'mean_absolute_percentage_error', 'symmetric_mean_absolute_percentage_error', 'spearmanr', 'pearsonr'])\n",
    "FSTestResults.to_csv('FSTestResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b786f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import shap\n",
    "import time\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogluonWrapper:\n",
    "    def __init__(self, predictor, feature_names):\n",
    "        self.ag_model = predictor\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = X.values.reshape(1,-1)\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.feature_names)\n",
    "        return self.ag_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edda298",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['CS'])\n",
    "X_test = test_data.drop(columns=['CS'])\n",
    "y_train = train_data['CS']\n",
    "y_test = test_data['CS']\n",
    "\n",
    "X_train_summary = shap.kmeans(X_train, 10)\n",
    "print(\"Baseline feature-values: \\n\", X_train_summary)\n",
    "def print_accuracy(f):\n",
    "    print(\"Root mean squared test error = {0}\".format(np.sqrt(np.mean((f(X_test) - y_test)**2))))\n",
    "    time.sleep(0.5) # to let the print get out before any progress bars\n",
    "feature_names = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_wrapper = AutogluonWrapper(predictor, feature_names)\n",
    "\n",
    "print_accuracy(ag_wrapper.predict)\n",
    "\n",
    "explainer = shap.KernelExplainer(ag_wrapper.predict, X_train_summary)\n",
    "\n",
    "NSHAP_SAMPLES = 1232  # how many samples to use to approximate each Shapely value, larger values will be slower\n",
    "N_VAL = 528\n",
    "# how many datapoints from validation data should we interpret predictions for, larger values will be slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce583123",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_INDEX = 0  # index of an example datapoint\n",
    "single_datapoint = X_train.iloc[[ROW_INDEX]]\n",
    "single_prediction = ag_wrapper.predict(single_datapoint)\n",
    "\n",
    "shap_values_single = explainer.shap_values(single_datapoint, nsamples=NSHAP_SAMPLES)\n",
    "shap.force_plot(explainer.expected_value, shap_values_single, X_train.iloc[ROW_INDEX,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test.iloc[0:N_VAL,:], nsamples=NSHAP_SAMPLES)\n",
    "shap.force_plot(explainer.expected_value, shap_values, X_test.iloc[0:N_VAL,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test.iloc[0:N_VAL,:], show=False)\n",
    "plt.savefig(\"SummaryPlotCS.tif\", dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433425d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.dependence_plot(\"ER\", shap_values, X_test.iloc[0:N_VAL,:], show=False)\n",
    "plt.savefig('ERdepenceplotforCS.tif', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ca616",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.dependence_plot(\"UPV\", shap_values, X_test.iloc[0:N_VAL,:], show=False)\n",
    "plt.savefig('UPVdepenceforCS.tif', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2d6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data['CS'] = y_test  # add labels to validation DataFrame\n",
    "predictor.feature_importance(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66827b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b92fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70319ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import autogluon.tabular\n",
    "\n",
    "# Load your trained AutoGluon model and data\n",
    "predictor = autogluon.tabular.TabularPredictor.load(\"AutoGluonModel\")\n",
    "X_test = test_data.drop(columns=[\"FS\"])\n",
    "\n",
    "# Use SHAP to explain predictions\n",
    "explainer = shap.Explainer(predictor.model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Visualize SHAP values\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(test_data, extra_metrics=['accuracy', 'balanced_accuracy', 'log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_model = 'KNeighborsDist'  # Replace with your model's name\n",
    "\n",
    "# Get predictions from the specified model\n",
    "model_predictions = predictor.predict(train_data, model=specific_model)\n",
    "predictor.evaluate(train_data, model=specific_model, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a81260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assuming 'predictor' is your trained AutoGluon predictor and 'test_data' is your test dataset\n",
    "# Evaluate the model to get predictions\n",
    "predictions = predictor.predict(train_data.drop(columns=['FS']), model=specific_model)  # Drop the target column for predictions\n",
    "actuals = train_data['FS'].values  # Get the actual values from the target column\n",
    "# Calculate a20-index\n",
    "def calculate_a20_index(actual, predicted):\n",
    "    within_20_percent = np.abs((actual - predicted) / actual) <= 0.20\n",
    "    m20 = np.sum(within_20_percent)\n",
    "    M = len(actual)\n",
    "    return (m20 / M) * 100\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "def calculate_mape(actual, predicted):\n",
    "    return np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "# Calculate Willmott’s Index (WI)\n",
    "def calculate_wi(actual, predicted):\n",
    "    numerator = np.sum((actual - predicted) ** 2)\n",
    "    denominator = np.sum((np.abs(predicted - np.mean(actual)) + np.abs(actual - np.mean(predicted))) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "# Calculate Nash-Sutcliffe Efficiency (NSE)\n",
    "def calculate_nse(actual, predicted):\n",
    "    return 1 - (np.sum((actual - predicted) ** 2) / np.sum((actual - np.mean(actual)) ** 2))\n",
    "# Compute MAPE\n",
    "mape = calculate_mape(actuals, predictions)\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape:.4f}%')\n",
    "# Compute a20-index\n",
    "a20_index = calculate_a20_index(actuals, predictions)\n",
    "print(f'a20-index: {a20_index:.4f}%')\n",
    "# Compute Willmott’s Index (WI)\n",
    "wi = calculate_wi(actuals, predictions)\n",
    "print(f'Willmott’s Index (WI): {wi:.4f}')\n",
    "# Compute Nash-Sutcliffe Efficiency (NSE)\n",
    "nse = calculate_nse(actuals, predictions)\n",
    "print(f'Nash-Sutcliffe Efficiency (NSE): {nse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3146a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['FS'])\n",
    "\n",
    "# Optionally, include the actual values and any identifier columns\n",
    "actuals = train_data['FS'].values\n",
    "predictions_df['Actual_FS'] = actuals\n",
    "predictions_df['ER'] = test_data['ER']  # If you have an identifier column in your test data\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "predictions_df.to_csv('predictioKNeighborsDist-FS-Train.csv', index=False)\n",
    "\n",
    "print('Predictions saved to predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7206e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_model = 'KNeighborsDist'  # Replace with your model's name\n",
    "\n",
    "# Get predictions from the specified model\n",
    "model_predictions = predictor.predict(test_data, model=specific_model)\n",
    "predictor.evaluate(test_data, model=specific_model, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.feature_importance(test_data, model=specific_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7725e4e6",
   "metadata": {},
   "source": [
    "predictor.feature_importance(test_data, model=specific_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b63a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'predictor' is your trained AutoGluon predictor and 'test_data' is your test dataset\n",
    "# Evaluate the model to get predictions\n",
    "predictions = predictor.predict(test_data.drop(columns=['FS']), model=specific_model)  # Drop the target column for predictions\n",
    "actuals = test_data['FS'].values  # Get the actual values from the target column\n",
    "\n",
    "# Calculate a20-index\n",
    "def calculate_a20_index(actual, predicted):\n",
    "    within_20_percent = np.abs((actual - predicted) / actual) <= 0.20\n",
    "    m20 = np.sum(within_20_percent)\n",
    "    M = len(actual)\n",
    "    return (m20 / M) * 100\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "def calculate_mape(actual, predicted):\n",
    "    return np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "# Calculate Willmott’s Index (WI)\n",
    "def calculate_wi(actual, predicted):\n",
    "    numerator = np.sum((actual - predicted) ** 2)\n",
    "    denominator = np.sum((np.abs(predicted - np.mean(actual)) + np.abs(actual - np.mean(predicted))) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "# Calculate Nash-Sutcliffe Efficiency (NSE)\n",
    "def calculate_nse(actual, predicted):\n",
    "    return 1 - (np.sum((actual - predicted) ** 2) / np.sum((actual - np.mean(actual)) ** 2))\n",
    "\n",
    "# Compute MAPE\n",
    "mape = calculate_mape(actuals, predictions)\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape:.4f}%')\n",
    "\n",
    "# Compute a20-index\n",
    "a20_index = calculate_a20_index(actuals, predictions)\n",
    "print(f'a20-index: {a20_index:.4f}%')\n",
    "\n",
    "# Compute Willmott’s Index (WI)\n",
    "wi = calculate_wi(actuals, predictions)\n",
    "print(f'Willmott’s Index (WI): {wi:.4f}')\n",
    "\n",
    "# Compute Nash-Sutcliffe Efficiency (NSE)\n",
    "nse = calculate_nse(actuals, predictions)\n",
    "print(f'Nash-Sutcliffe Efficiency (NSE): {nse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f597e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['FS'])\n",
    "\n",
    "# Optionally, include the actual values and any identifier columns\n",
    "actuals = test_data['FS'].values\n",
    "predictions_df['Actual_FS'] = actuals\n",
    "predictions_df['ER'] = test_data['ER']  # If you have an identifier column in your test data\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "predictions_df.to_csv('predictionsKNeighborsDist-FS-Test.csv', index=False)\n",
    "\n",
    "print('Predictions saved to predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here’s how to specify a particular model to use for prediction instead of AutoGluon’s default model-choice:\n",
    "i = 5  # index of model to use\n",
    "model_to_use = predictor.model_names()[i]\n",
    "model_pred = predictor.predict(test_data, model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = predictor.model_names()\n",
    "model_to_use = all_models[i]\n",
    "specific_model = predictor._trainer.load_model(model_to_use)\n",
    "\n",
    "# Objects defined below are dicts of various information (not printed here as they are quite large):\n",
    "model_info = specific_model.get_info()\n",
    "predictor_information = predictor.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089df377",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can easily access various information about the trained predictor or a particular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'predictor' is your trained AutoGluon predictor and 'train_data' is your train dataset\n",
    "# Get the predictions\n",
    "predictions = predictor.predict(train_data.drop(columns=['UCS']))  # Drop the target column for predictions\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['UCS'])\n",
    "\n",
    "# Optionally, include the actual values and any identifier columns\n",
    "actuals = train_data['UCS'].values\n",
    "predictions_df['Actual_UCS'] = actuals\n",
    "predictions_df['Curing_days'] = train_data['Curing_days']  # If you have an identifier column in your train data\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "predictions_df.to_csv('predictionsAutoMLTrainIsoForest.csv', index=False)\n",
    "\n",
    "print('Predictions saved to predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'predictor' is your trained AutoGluon predictor and 'test_data' is your test dataset\n",
    "# Get the predictions\n",
    "predictions = predictor.predict(test_data.drop(columns=['CS']))  # Drop the target column for predictions\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['CS'])\n",
    "\n",
    "# Optionally, include the actual values and any identifier columns\n",
    "actuals = test_data['CS'].values\n",
    "predictions_df['Actual_CS'] = actuals\n",
    "predictions_df['CD'] = test_data['CD']  # If you have an identifier column in your test data\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "predictions_df.to_csv('predictionsAutoMLTest42.csv', index=False)\n",
    "\n",
    "print('Predictions saved to predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'predictor' is your trained AutoGluon predictor and 'test_data' is your test dataset\n",
    "# Evaluate the model to get predictions\n",
    "predictions = predictor.predict(test_data.drop(columns=['UCS']))  # Drop the target column for predictions\n",
    "actuals = test_data['UCS'].values  # Get the actual values from the target column\n",
    "\n",
    "\n",
    "# Calculate a20-index\n",
    "def calculate_a20_index(actual, predicted):\n",
    "    within_20_percent = np.abs((actual - predicted) / actual) <= 0.20\n",
    "    m20 = np.sum(within_20_percent)\n",
    "    M = len(actual)\n",
    "    return (m20 / M) * 100\n",
    "\n",
    "# Compute MAPE\n",
    "mape = calculate_mape(actuals, predictions)\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape:.4f}%')\n",
    "\n",
    "# Compute a20-index\n",
    "a20_index = calculate_a20_index(actuals, predictions)\n",
    "print(f'a20-index: {a20_index:.4f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6238f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d104cf0-3092-496c-ae63-1517fb2da067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
